Automatically generated by Mendeley Desktop 1.17.12
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Chang2001,
author = {Chang, C.H. and Chang, C.H. and Lui, S.C. and Lui, S.C.},
file = {:Users/joaomateusdefreitasveneroso/Books/10.1.1.17.9236.pdf:pdf},
isbn = {1581133480},
journal = {Proceedings of the 10th international conference on World Wide Web},
keywords = {extraction rule,information extraction,multiple string,pat tree},
pages = {681--688},
title = {{IEPAD: information extraction based on pattern discovery}},
year = {2001}
}
@article{Liu2003,
abstract = {A large amount of information on the Web is contained in regularly structured objects, which we call data records. Such data records are important because they often present the essential information of their host pages, e.g., lists of products or services. It is useful to mine such data records in order to extract information from them to provide value-added services. Existing automatic techniques are not satisfactory because of their poor accuracies. In this paper, we propose a more effective technique to perform the task. The technique is based on two observations about data records on the Web and a string matching algorithm. The proposed technique is able to mine both contiguous and non-contiguous data records. Our experimental results show that the proposed technique outperforms existing techniques substantially.},
author = {Liu, Bing and Grossman, Robert},
doi = {10.1145/956804.956826},
file = {:Users/joaomateusdefreitasveneroso/Books/10.1.1.3.661.pdf:pdf},
isbn = {1581137370},
journal = {Knowledge discovery and data mining},
keywords = {web data records,web information integration,web mining},
pages = {601},
title = {{Mining data records in Web pages}},
url = {http://portal.acm.org/citation.cfm?id=956826},
year = {2003}
}
@article{Chang2004,
abstract = {OLERA is a semisupervised information-extraction system that produces extraction rules from semistructured Web documents without requiring detailed annotation of the training documents. It performs well for program-generated Web pages with few training pages and limited user intervention.},
author = {Chang, Chia Hui and Kuo, Shih Chien},
doi = {10.1109/MIS.2004.71},
file = {:Users/joaomateusdefreitasveneroso/Books/f5cd214f2a9f70f685b9f36ffd19c520d947.pdf:pdf},
issn = {15411672},
journal = {IEEE Intelligent Systems},
number = {6},
pages = {56--64},
title = {{OLERA: Semisupervised Web-data extraction with visual support}},
volume = {19},
year = {2004}
}
@article{Hogue2005,
abstract = {We describe Thresher, a system that lets non-technical users teach their browsers how to extract semantic web content from HTML documents on the World Wide Web. Users specify examples of semantic content by highlighting them in a web browser and describing their meaning. We then use the tree edit distance between the DOM subtrees of these ex- amples to create a general pattern, or wrapper, for the con- tent, and allow the user to bind RDF classes and predicates to the nodes of these wrappers. By overlaying matches to these patterns on standard documents inside the Haystack semantic web browser, we enable a rich semantic interac- tion with existing web pages, “unwrapping” semantic data buried in the pages' HTML. By allowing end-users to cre- ate, modify, and utilize their own patterns, we hope to speed adoption and use of the Semantic Web and its applications.},
author = {Hogue, Andrew and Karger, David},
doi = {10.1145/1060745.1060762},
file = {:Users/joaomateusdefreitasveneroso/Books/e9b2ea7560d65cfa97941bfff8e9ab7debb4.pdf:pdf},
isbn = {1595930469},
journal = {WWW '05: Proceedings of the 14th international conference on World Wide Web},
keywords = {haystack,rdf,semantic web,tree edit,wrapper induction},
pages = {86--95},
title = {{Thresher : Automating the Unwrapping of Semantic Content from the World Wide Web}},
year = {2005}
}
@article{Wang2003,
abstract = {Many tools have been developed to help users query, extract and integrate data from web pages generated dynamically from databases, i.e., from the Hidden Web. A key prerequisite for such tools is to obtain the schema of the attributes of the retrieved data. In this paper, we describe a system called, DeLa, which reconstructs (part of) a "hidden" back-end web database. It does this by sending queries through HTML forms, automatically generating regular expression wrappers to extract data objects from the result pages and restoring the retrieved data into an annotated (labelled) table. The whole process needs no human involvement and proves to be fast (less than one minute for wrapper induction for each site) and accurate (over 90{\%} correctness for data extraction and around 80{\%} correctness for label assignment).},
author = {Wang, Jiying and Lochovsky, Fred H.},
doi = {10.1145/775177.775179},
file = {:Users/joaomateusdefreitasveneroso/Books/p187-wang.pdf:pdf},
isbn = {1581136803},
issn = {00320862},
journal = {Proceedings of the twelfth international conference on World Wide Web - WWW '03},
pages = {187},
title = {{Data extraction and label assignment for web databases}},
url = {http://portal.acm.org/citation.cfm?doid=775152.775179},
year = {2003}
}
@article{Furche2012,
abstract = {The extraction of multi-attribute objects from the deep web is the bridge between the unstructured web and structured data. Existing approaches either induce wrappers from a set of human-annotated pages or leverage repeated structures on the page without supervision. What the former lack in automation, the latter lack in accuracy. Thus accurate, automatic multi-attribute object extraction has remained an open challenge. AMBER overcomes both limitations through mutual supervision between the repeated structure and automatically produced annotations. Previous approaches based on automatic annotations have suffered from low quality due to the inherent noise in the annotations and have attempted to compensate by exploring multiple candidate wrappers. In contrast, AMBER compensates for this noise by integrating repeated structure analysis with annotation-based induction: The repeated structure limits the search space for wrapper induction, and conversely, annotations allow the repeated structure analysis to distinguish noise from relevant data. Both, low recall and low precision in the annotations are mitigated to achieve almost human quality (more than 98 percent) multi-attribute object extraction. To achieve this accuracy, AMBER needs to be trained once for an entire domain. AMBER bootstraps its training from a small, possibly noisy set of attribute instances and a few unannotated sites of the domain.},
archivePrefix = {arXiv},
arxivId = {1210.5984},
author = {Furche, Tim and Gottlob, Georg and Grasso, Giovanni and Orsi, Giorgio and Schallhart, Christian and Wang, Cheng},
eprint = {1210.5984},
file = {:Users/joaomateusdefreitasveneroso/Books/AMBER{\_}Automatic{\_}Supervision{\_}for{\_}Multi-Attribute{\_}Ex.pdf:pdf},
journal = {arXiv preprint},
number = {5984},
pages = {1--22},
title = {{AMBER: Automatic Supervision for Multi-Attribute Extraction}},
url = {http://arxiv.org/abs/1210.5984},
volume = {1210},
year = {2012}
}
@article{Schulz2016,
author = {Schulz, Andreas and L{\"{a}}ssig, J{\"{o}}rg and Gaedke, Martin},
doi = {10.1109/WI.2016.95},
file = {:Users/joaomateusdefreitasveneroso/Books/07817113.pdf:pdf},
isbn = {9781509044702},
journal = {IEEE/WIC/ACM International Conference on Web Intelligence (WI), 2016},
pages = {562----567},
title = {{Practical web data extraction: Are we there yet? --- A short survey}},
year = {2016}
}
@article{Garfield1955a,
annote = {Garfield proposes an index of citations per published article in which the "impact factor" of an article could be measured by the amount of citations it received. However he does not describe any formula to calculate this "impact factor".},
author = {Garfield, Eugene},
file = {:Users/joaomateusdefreitasveneroso/Library/Application Support/Mendeley Desktop/Downloaded/Garfield - 1955 - Citation Indexes for Science.pdf:pdf},
journal = {Science},
pages = {108--111},
title = {{Citation Indexes for Science}},
volume = {122},
year = {1955}
}
@article{Su2009,
author = {Su, Weifeng and Lochvsky, Frederick H},
file = {:Users/joaomateusdefreitasveneroso/Books/10.1.1.172.1205.pdf:pdf},
number = {212},
title = {{ODE : Ontology-assisted Data Extraction}},
volume = {1},
year = {2009}
}
@article{Laender2002a,
author = {Laender, Alberto H F and Ribeiro-Neto, Berthier and da Silva, Altigran S},
doi = {http://dx.doi.org/10.1016/S0169-023X(01)00047-7},
file = {:Users/joaomateusdefreitasveneroso/Books/1-s2.0-S0169023X01000477-main.pdf:pdf},
issn = {0169-023X},
journal = {Data Knowl. Eng.},
keywords = {data extraction,web data management,wrapper generation},
number = {2},
pages = {121--154},
title = {{DEByE - Date extraction by example}},
volume = {40},
year = {2002}
}
@article{Freitag1998,
author = {Freitag, Dayne},
file = {:Users/joaomateusdefreitasveneroso/Books/freitag98information.pdf:pdf},
journal = {Proceedings of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence},
pages = {517--523},
title = {{Information Extraction from HTML: Application of a General Machine Learning Approach}},
year = {1998}
}
@article{Kushmerick1997,
abstract = {Many Internet information resources present relational data--telephone directories, product catalogs, etc. Because these sites are formatted for people, mechanically extracting their content is difficult. Systems using such resources typically use hand-coded wrappers, procedures to extract data from information resources. We introduce wrapper induction, a method for automatically constructing wrappers, and identify HLRT, a wrapper class that is efficiently learnable, yet expressive enough to handle 48{\%} of a recently surveyed sample of Internet resources. We use PAC analysis to bound the problem's sample complexity, and show that the system degrades gracefully with imperfect labeling knowledge.},
author = {Kushmerick, Nicholas and Weld, Daniel S and Doorenbos, R},
doi = {10.1.1.33.2176},
file = {:Users/joaomateusdefreitasveneroso/Books/UW-CSE-97-11-04.pdf:pdf},
isbn = {0591708434},
journal = {Intl Joint Conference on Artificial Intelligence IJCAI},
pages = {729--737},
title = {{Wrapper induction for information extraction}},
url = {citeseer.ist.psu.edu/kushmerick97wrapper.html},
year = {1997}
}
@article{Abdessalem2010,
abstract = {We present in this paper ObjectRunner, a system for extracting, integrating and querying structured data from the Web. Our system harvests real-world items from template-based HTML pages (the so-called structured Web). It illustrates a two-phase querying of the Web, in which an intentional description of the targeted data is first provided, in a flexible and widely applicable manner. ObjectRunner follows then a lightweight, best-effort approach, leveraging both the input description and the source structure. This process is domain-independent, in the sense that it applies to any relation, either flat or nested, describing real-world items. We advocate via our prototype that fully automatic extraction and integration of structured data can be done fast and effectively, when the redundancy of the Web meets knowledge over the to-be-extracted data. We present the technical details and the overall platform through several application scenarios on real-life Web sources.},
author = {Abdessalem, Talel and Cautis, B and Derouiche, Nora},
file = {:Users/joaomateusdefreitasveneroso/Books/D18.pdf:pdf},
issn = {21508097},
journal = {Proceedings of the VLDB {\ldots}},
number = {2},
pages = {1585--1588},
title = {{ObjectRunner: lightweight, targeted extraction and querying of structured web data}},
url = {http://dl.acm.org/citation.cfm?id=1921045},
volume = {3},
year = {2010}
}
@article{Kaiser2005,
abstract = {Information Extraction is a technique used to detect relevant information in larger docu- ments and present it in a structured format. Information Extraction is not Text Understand- ing. It is used to analyze the text and locate specific pieces of information in the text. Information Extraction techniques can be applied to structured, semi-structured, and unstructured texts. For the latter one, Natural Language Processing is necessary which is implemented in traditional Information Extraction systems. To process structured and semi-structured texts often no NLP techniques are necessary as they do not offer such a rich grammatical structure. For this reason, so called wrappers are developed that incorporate the different structures of documents. In this paper we will describe the requirements and components of Information Extrac- tion systems as well as present various approaches for building such systems. We then will represent important methodologies and systems for both traditional Information Extraction systems and wrapper generation systems.},
author = {Kaiser, Katharina and Miksch, Silvia},
file = {:Users/joaomateusdefreitasveneroso/Books/pub-inf{\_}2999.pdf:pdf},
journal = {Technology},
number = {May},
pages = {32},
title = {{Information Extraction}},
url = {http://ieg.ifs.tuwien.ac.at/techreports/Asgaard-TR-2005-6.pdf},
year = {2005}
}
@article{Califf1999,
abstract = {Information extraction systems process natural language documents and locate a specific set of relevant items. Given the recent success of empirical or corpus- based approaches in other areas of natu- ral language processing, machine learning has the potential to significantly aid the development of these knowledge-intensive systems. This paper presents a system, RAPmrt, that takes pairs of documents and filled templates and induces pattern-match rules that directly extract fillers for the slots in the template. The learning al- gorithm incorporates techniques from sev- eral inductive logic programming systems and learns unbounded patterns that in- clude constraints on the words and part- of-speech tags surrounding the filler. En- couraging results are presented on learn- ing to extract information from com- puter job postings from the newsgroup misc. jobs. offered.},
author = {Califf, Mary Elaine and Mooney, Raymond J},
doi = {10.1162/153244304322972685},
file = {:Users/joaomateusdefreitasveneroso/Books/10.1.1.11.3310.pdf:pdf},
isbn = {0262511061},
issn = {15324435},
journal = {Computational Linguistics},
pages = {9--15},
title = {{Relational learning of pattern-match rules for information extraction}},
url = {http://acl.ldc.upenn.edu/W/W97/W97-1002.pdf},
volume = {4},
year = {1999}
}
@article{Adelberg1998,
abstract = {Often interesting structured or semistructured data is not in database systems but in HTML pages, text files, or on paper. The data in these formats is not usable by standard query processing engines and hence users need a way of extracting data from these sources into a DBMS or of writing wrappers around the sources. This paper describes NoDoSE, the Northwestern Document Structure Extractor, which is an interactive tool for semi-automatically determining the structure of such documents and then extracting their data. Using a GUI, the user hierarchically decomposes the file, outlining its interesting regions and then describing their semantics. This task is expedited by a mining component that attempts to infer the grammar of the file from the information the user has input so far. Once the format of a document has been determined, its data can be extracted into a number of useful forms. This paper describes both the NoDoSE architecture, which can be used as a test bed for structure mining algorithms in general, and the mining algorithms that have been developed by the author. The prototype, which is written in Java, is described and experiences parsing a variety of documents are reported.},
author = {Adelberg, Brad},
doi = {10.1145/276305.276330},
file = {:Users/joaomateusdefreitasveneroso/Books/p283-adelberg.pdf:pdf},
isbn = {0-89791-995-5},
issn = {01635808},
journal = {ACM SIGMOD Record},
number = {2},
pages = {283--294},
title = {{NoDoSE---a tool for semi-automatically extracting structured and semistructured data from text documents}},
url = {http://portal.acm.org/citation.cfm?doid=276305.276330},
volume = {27},
year = {1998}
}
@article{Hammer1997,
author = {Hammer, Joachim and Mchugh, Jason and Garcia-molina, Hector},
file = {:Users/joaomateusdefreitasveneroso/Books/ewic{\_}ad97{\_}paper22.pdf:pdf},
journal = {Proceedings of the First East-European Symposium on Advances in Databases and Information Systems},
pages = {1--8},
title = {{Semistructured Data : The TSIMMIS Experience}},
year = {1997}
}
@article{Flesca2004,
abstract = {Nowadays several companies use the information available on the Web for a number of purposes. However, since most of this information is only available as HTML documents, several techniques that allow information from the Web to be automatically extracted have recently been defined. In this paper we review the main techniques and tools for extracting information available on the Web, devising a taxonomy of existing systems. In particular we emphasize the advantages and drawbacks of the techniques analyzed from a user point of view.},
author = {Flesca, S. and Manco, G. and Masciari, E. and Rende, E. and Tagarelli, A.},
file = {:Users/joaomateusdefreitasveneroso/Books/5b.aicom{\_}Flesca.pdf:pdf},
issn = {0921-7126},
journal = {AI Communications},
keywords = {information extraction,wrapper genera-},
number = {2},
pages = {57--61},
title = {{Web wrapper induction: a brief survey}},
url = {http://portal.acm.org/citation.cfm?id=1218707},
volume = {17},
year = {2004}
}
@article{Arasu2003,
abstract = {Many web sites contain large sets of pages generated using a common template or layout. For example, Amazon lays out the author, title, comments, etc. in the same way in all its book pages. The values used to generate the pages (e.g., the author, title,...) typically come from a database. In this paper, we study the problem of automatically extracting the database values from such template-generated web pages without any learning examples or other similar human input. We formally define a template, and propose a model that describes how values are encoded into pages using a template. We present an algorithm that takes, as input, a set of template-generated pages, deduces the unknown template used to generate the pages, and extracts, as output, the values encoded in the pages. Experimental evaluation on a large number of real input page collections indicates that our algorithm correctly extracts data in most cases.},
author = {Arasu, A. and Garcia-Molina, H. and Arasu, Arvind and Garcia-Molina, Hector},
doi = {10.1145/872797.872799},
file = {:Users/joaomateusdefreitasveneroso/Books/extract.pdf:pdf},
isbn = {158113634X},
journal = {2003 ACM SIGMOD International Conference on Management of Data},
pages = {337 -- 348},
title = {{Extracting structured data from Web pages}},
url = {http://portal.acm.org/citation.cfm?doid=872757.872799},
year = {2003}
}
@article{Krupl2005,
abstract = {We describe a method to extract tabular data from web pages. Rather than just analyzing the DOM tree, we also exploit visual cues in the rendered version of the document to extract data from tables which are not explicitly marked with an HTML table element. To detect tables, we rely on a variant of the well-known X-Y cut algorithm as used in the OCR community. We implemented the system by directly accessing Mozilla's box model that contains the positional data for all HTML elements of a given web page.},
author = {Kr{\"{u}}pl, Bernhard and Herzog, Marcus and Gatterbauer, Wolfgang},
doi = {10.1145/1062745.1062838},
file = {:Users/joaomateusdefreitasveneroso/Books/10.1.1.94.7698.pdf:pdf},
isbn = {1595930515},
journal = {Special interest tracks and posters of the 14th international conference on World Wide Web - WWW '05},
keywords = {table detection,visual analysis,web information extraction},
pages = {1000----1001},
title = {{Using visual cues for extraction of tabular data from arbitrary HTML documents}},
url = {http://portal.acm.org/citation.cfm?doid=1062745.1062838},
year = {2005}
}
@article{Laender2002,
author = {Laender, A.H.F. and Ribeiro-Neto, B. A. and S.Teixeria, Juliana},
file = {:Users/joaomateusdefreitasveneroso/Books/laender-survey.pdf:pdf},
journal = {ACM SIGMOD Record 31(2)},
pages = {84--93},
title = {{A brief survey of web data extraction tools}},
year = {2002}
}
@article{Dong2014,
abstract = {Recent years have witnessed a proliferation of large-scale knowledge bases, includingWikipedia, Freebase, YAGO,Mi- crosoft's Satori, and Google's Knowledge Graph. To in- crease the scale even further, we need to explore automatic methods for constructing knowledge bases. Previous ap- proaches have primarily focused on text-based extraction, which can be very noisy. Here we introduce Knowledge Vault, a Web-scale probabilistic knowledge base that com- bines extractions fromWeb content (obtained via analysis of text, tabular data, page structure, and human annotations) with prior knowledge derived fromexisting knowledge repos- itories. We employ supervised machine learning methods for fusing these distinct information sources. The Knowledge Vault is substantially bigger than any previously published structured knowledge repository, and features a probabilis- tic inference system that computes calibrated probabilities of fact correctness. We report the results of multiple studies that explore the relative utility of the different information sources and extraction methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Dong, Xin and Gabrilovich, Evgeniy and Heitz, Geremy and Horn, Wilko and Lao, Ni and Murphy, Kevin and Strohmann, Thomas and Sun, Shaohua and Zhang, Wei},
doi = {10.1145/2623330.2623623},
eprint = {arXiv:1301.3781v3},
file = {:Users/joaomateusdefreitasveneroso/Books/kv-kdd14.pdf:pdf},
isbn = {9781450329569},
issn = {0893-6080},
journal = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '14},
keywords = {information extraction,knowledge bases,machine learning,probabilistic models},
pages = {601--610},
pmid = {1975973},
title = {{Knowledge vault: a web-scale approach to probabilistic knowledge fusion}},
url = {http://dl.acm.org/citation.cfm?doid=2623330.2623623},
year = {2014}
}
@article{Sarawagi2008,
abstract = {The automatic extraction of information from unstructured sources has opened up new avenues for querying, organizing, and analyzing data by drawing upon the clean semantics of structured databases and the abundance of unstructured data. The field of information extraction has its genesis in the natural language processing community where the primary impetus came from competitions centered around the recog- nition of named entities like people names and organization from news articles. As society became more data oriented with easy online access to both structured and unstructured data, new applications of struc- ture extraction came around. Now, there is interest in converting our personal desktops to structured databases, the knowledge in scien- tific publications to structured records, and harnessing the Internet for structured fact finding queries. Consequently, there are many different communities of researchers bringing in techniques from machine learn- ing, databases, information retrieval, and computational linguistics for various aspects of the information extraction problem. This review is a survey of information extraction research of over two decades from these diverse communities. We},
author = {Sarawagi, Sunita},
doi = {10.1561/1500000003},
file = {:Users/joaomateusdefreitasveneroso/Books/10.1.1.164.2388.pdf:pdf},
isbn = {9783540926733},
issn = {1931-7883},
journal = {Foundations and Trends in Databases},
keywords = {in,ndations and trends r},
number = {3},
pages = {261--377},
pmid = {8806820},
title = {{Information extraction}},
url = {http://dl.acm.org/citation.cfm?id=234209},
volume = {1},
year = {2008}
}
@article{Zhai2005,
abstract = {This paper studies the problem of extracting data from a Web page that contains several structured data records. The objective is to segment these data records, extract data items/fields from them and put the data in a database table. This problem has been studied by several researchers. However, existing methods still have some serious limitations. The first class of methods is based on machine learning, which requires human labeling of many examples from each Web site that one is interested in extracting data from. The process is time consuming due to the large number of sites and pages on the Web. The second class of algorithms is based on automatic pattern discovery. These methods are either inaccurate or make many assumptions. This paper proposes a new method to perform the task automatically. It consists of two steps, (1) identifying individual data records in a page, and (2) aligning and extracting data items from the identified data records. For step 1, we propose a method based on visual information to segment data records, which is more accurate than existing methods. For step 2, we propose a novel partial alignment technique based on tree matching. Partial alignment means that we align only those data fields in a pair of data records that can be aligned (or matched) with certainty, and make no commitment on the rest of the data fields. This approach enables very accurate alignment of multiple data records. Experimental results using a large number of Web pages from diverse domains show that the proposed two-step technique is able to segment data records, align and extract data from them very accurately.},
author = {Zhai, Yanhong and Liu, Bing},
doi = {10.1145/1060745.1060761},
file = {:Users/joaomateusdefreitasveneroso/Books/10.1.1.66.277.pdf:pdf},
isbn = {1595930469},
issn = {10414347},
journal = {Proceedings of the 14th international conference on World Wide Web - WWW '05},
keywords = {data extraction,data record extraction,wrapper},
pages = {76},
pmid = {17392084},
title = {{Web data extraction based on partial tree alignment}},
url = {http://portal.acm.org/citation.cfm?doid=1060745.1060761},
year = {2005}
}
@article{Doan2008,
abstract = {Over the past few years, we have been trying to build an end-to-end system at Wisconsin to manage unstructured data, using extraction, integration, and user interaction. This paper describes the key information extraction (IE) challenges that we have run into, and sketches our solutions. We discuss in particular developing a declarative IE language, optimizing for this language, generating IE provenance, incorporating user feedback into the IE process, developing a novel wiki-based user interface for feedback, best-effort IE, pushing IE into RDBMSs, and more. Our work suggests that IE in managing unstructured data can open up many interesting research challenges, and that these challenges can greatly benefit from the wealth of work on managing structured data that has been carried out by the database community.},
author = {Doan, Anhai and Naughton, Jeffrey F and Ramakrishnan, Raghu and Baid, Akanksha and Chai, Xiaoyong and Chen, Fei and Chen, Ting and Chu, Eric and Derose, Pedro and Gao, Byron and Gokhale, Chaitanya and Huang, Jiansheng and Shen, Warren and Vuong, Ba-quy},
doi = {10.1145/1519103.1519106},
file = {:Users/joaomateusdefreitasveneroso/Books/10.1.1.153.3589.pdf:pdf},
isbn = {0163-5808},
issn = {0163-5808},
journal = {ACM SIGMOD Record},
keywords = {and the user services,can also solicit user,extrac-,ing database,interaction to improve the,result-,search and structured querying,such systems,the quality of the,tion and integration methods},
number = {4},
pages = {14--20},
title = {{Information Extraction Challenges in Managing Unstructured Data}},
volume = {37},
year = {2008}
}
@article{Chang2012,
author = {Chang, Chia-hui},
file = {:Users/joaomateusdefreitasveneroso/Books/FiVaTech2{\_}A{\_}Supervised{\_}Approach{\_}to{\_}Role{\_}Differenti.pdf:pdf},
number = {March 2015},
title = {{FiVaTech2 : A Supervised Approach to Role Differentiation for Web Data Extraction From Template Pages}},
year = {2012}
}
@article{Crescenzi2001,
abstract = {The paper investigates techniques for extracting data from HTML sites through the use of automatically generated wrappers. To automate the wrapper generation and the data extraction process, the paper develops a novel technique to compare HTML pages and generate a wrapper based on their similarities and di{\#}erences. Experimental results on real-life data-intensive Web sites confirm the feasibility of the approach.},
archivePrefix = {arXiv},
arxivId = {10.1.1.21.8672},
author = {Crescenzi, Valter and Mecca, Giansalvatore and Merialdo, Paolo},
doi = {10.1.1.21.8672},
eprint = {10.1.1.21.8672},
file = {:Users/joaomateusdefreitasveneroso/Books/10.1.1.21.8672.pdf:pdf},
isbn = {1558608044},
issn = {10477349},
journal = {Proceedings of the 27th International Conference on Very Large Data Bases},
pages = {109--118},
title = {{Roadrunner: Towards automatic data extraction from large web sites}},
url = {http://www.vldb.org/conf/2001/P109.pdf},
year = {2001}
}
@article{Kushmerick2003,
abstract = {Information agents are emerging as an important approach to building next- generation value-added information services. An information agent is a distributed system that receives a goal through its user interface, gathers information relevant to this goal from a variety of sources, processes this content as appropriate,and delivers the results to the users. We focus on the second stage in this generic architecture. We survey a variety of information extraction techniques that enable information agents to automatically gather information from heterogeneous sources.},
author = {Kushmerick, Nicholas and Kushmerick, Nicholas},
doi = {10.1007/978-3-540-45092-4_4},
file = {:Users/joaomateusdefreitasveneroso/Books/cfaa2efec87badddd78f5ca2c7ef9f4d5e1e.pdf:pdf},
isbn = {978-3-540-40579-5},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
pages = {77--91},
title = {{Finite-state approaches to Web information extraction}},
url = {http://www.springerlink.com/index/016HNJ50N87PDRHV.pdf},
volume = {2700},
year = {2003}
}
@article{Ferrara2011,
abstract = {Information distributed through the Web keeps growing faster day by day, and for this reason, several techniques for extracting Web data have been suggested during last years. Often, extraction tasks are performed through so called wrappers, procedures extracting information from Web pages, e.g. implementing logic-based techniques. Many fields of application today require a strong degree of robustness of wrappers, in order not to compromise assets of information or reliability of data extracted. Unfortunately, wrappers may fail in the task of extracting data from a Web page, if its structure changes, sometimes even slightly, thus requiring the exploiting of new techniques to be automatically held so as to adapt the wrapper to the new structure of the page, in case of failure. In this work we present a novel approach of automatic wrapper adaptation based on the measurement of similarity of trees through improved tree edit distance matching techniques.},
archivePrefix = {arXiv},
arxivId = {1103.1252},
author = {Ferrara, Emilio and Baumgartner, Robert},
doi = {10.1007/978-3-642-19618-8_3},
eprint = {1103.1252},
file = {:Users/joaomateusdefreitasveneroso/Books/1103.1252.pdf:pdf},
isbn = {9783642196171},
issn = {21903018},
journal = {Smart Innovation, Systems and Technologies},
pages = {41--54},
title = {{Automatic wrapper adaptation by tree edit distance matching}},
volume = {8},
year = {2011}
}
@article{Sahuguet1999,
author = {Sahuguet, Arnaud and Azavant, Fabien},
file = {:Users/joaomateusdefreitasveneroso/Books/6020d376dd28d4b1d3598c16e9477f379113.pdf:pdf},
journal = {Proceedings of the 25th VLDB Conference},
pages = {738--741},
title = {{Building light-weight wrappers for legacy Web data-sources using W4F}},
volume = {99},
year = {1999}
}
@article{Shi2015,
abstract = {A Web database typically responds to a query with a Web page, which encodes the query results into semi-structured data objects using HTML tags. We call such data objects Web data records or data records. Mining Web data records is very important for many applications, e.g., meta search, comparative shopping, etc. This paper proposes a new effective approach called AutoRM, which mines data records from single Web page automatically. AutoRM involves three major steps: (1) constructing the DOM tree of the given Web page; (2) mining all sets of adjacent similar C-Records (Candidate data Records) from the constructed DOM tree; (3) mining actual data records from C-Records. In many Web pages, similar data records are distributed in bigger and adjacent similar objects. Existing approaches typically identify such objects as data records. Conversely, AutoRM views such objects as C-Records, and mines actual data records from them. One key issue for mining similar data records is the boundary detection of each data record. Existing approaches typically make some brittle assumptions for handling this issue. By making more robust assumptions, AutoRM tends to detect data record boundaries more accurately. Experimental results show that AutoRM is highly effective, and outperforms state-of-the-art approaches.},
author = {Shi, Shengsheng and Liu, Chengfei and Shen, Yi and Yuan, Chunfeng and Huang, Yihua},
doi = {10.1016/j.knosys.2015.07.012},
file = {:Users/joaomateusdefreitasveneroso/Books/1-s2.0-S095070511500266X-main.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Data record mining,Web data extraction,Web mining},
pages = {314--331},
publisher = {Elsevier B.V.},
title = {{AutoRM: An effective approach for automatic Web data record mining}},
url = {http://dx.doi.org/10.1016/j.knosys.2015.07.012},
volume = {89},
year = {2015}
}
@article{Figueiredo2017,
author = {Figueiredo, Leandro Neiva Lopes and de Assis, Guilherme Tavares and Ferreira, Anderson A.},
doi = {10.1016/j.ipm.2017.04.007},
file = {:Users/joaomateusdefreitasveneroso/Books/1-s2.0-S0306457316300516-main.pdf:pdf},
issn = {03064573},
journal = {Information Processing {\&} Management},
keywords = {Rendering information,Visual information,Wrapper,M,rendering information},
number = {5},
pages = {1120--1138},
publisher = {Elsevier Ltd},
title = {{DERIN: A data extraction method based on rendering information and n-gram}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0306457316300516},
volume = {53},
year = {2017}
}
@article{Varlamov2016,
author = {Varlamov, M. I. and Turdakov, D. Yu.},
doi = {10.1134/S0361768816050078},
file = {:Users/joaomateusdefreitasveneroso/Books/10.1134{\%}2FS0361768816050078.pdf:pdf},
issn = {0361-7688},
journal = {Programming and Computer Software},
number = {5},
pages = {279--291},
title = {{A survey of methods for the extraction of information from Web resources}},
url = {http://link.springer.com/10.1134/S0361768816050078},
volume = {42},
year = {2016}
}
@article{Fiumara2007,
abstract = {TheWeb contains an enormous quantity of information which is usually formatted for human users. This makes it difficult to extract relevant content from various sources. In the last few years some authors have addressed the problem to convert Web documents from unstructured or semi-structured format into struc- tured and therefore machine-understandable format such as, for example, XML. In this paper we briefly survey some of the most promising and recently developed extraction tools.},
author = {Fiumara, Giacomo},
doi = {10.1.1.84.7204},
file = {:Users/joaomateusdefreitasveneroso/Books/bof07-fiumara.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
pages = {1--9},
title = {{Automated information extraction from Web sources: A survey}},
volume = {312},
year = {2007}
}
@article{Muslea1999,
author = {Muslea, Ion and Minton, Steve and Knoblock, Craig},
file = {:Users/joaomateusdefreitasveneroso/Books/muslea99-agents.pdf:pdf},
journal = {Proc. of the Third Annual Conf. on Autonomous Agents, ACM},
pages = {190--197},
title = {{A Hierarchical Approach to Wrapper Induction}},
year = {1999}
}
@article{Kao2010,
abstract = {Blog posts containing many personal experiences or perspectives toward specific subjects are useful. Blogs allow readers to interact with bloggers by placing comments on specific blog posts. The comments carry viewpoints of readers toward the targets described in the post, or supportive/non-supportive attitude toward the post. Comment extraction is challenging due to that there does not exist a unique template among all blog service providers. This paper proposes methods to deal with this problem. Firstly, the repetitive patterns and their corresponding blocks are extracted from input posts by pattern identification algorithm. Secondly, three filtering strategies, i.e., tag pattern loop filtering, rule overlap filtering, and longest rule first, are used to remove non-comment blocks. Finally, a comment/non-comment classifier is learned to distinguish comment blocks from non-comment blocks with 14 block-level features and 5 rule-level features. In the experiments, we randomly select 600 blog posts from 12 blog service providers. F-measure, recall, and precision are 0.801, 0.855, and 0.780, respectively, by using all of the three filtering strategies together with some selected features. The application of comment extraction to blog mining is also illustrated. We show how to identify the relevant opinionated objects ― say, opinion holders, opinions, and targets, from posts.},
author = {Kao, Huan-An and Chen, Hsin-Hsi},
file = {:Users/joaomateusdefreitasveneroso/Books/17{\_}Paper.pdf:pdf},
isbn = {2-9517408-6-7},
journal = {Proceedings of the 7th International Conference on Language Resources and Evaluation},
pages = {1113--1120},
title = {{Comment Extraction from Blog Posts and Its Applications to Opinion Mining}},
url = {http://www.lrec-conf.org/proceedings/lrec2010/pdf/17{\_}Paper.pdf},
year = {2010}
}
@article{Chang2006,
abstract = {The Internet presents a huge amount of useful information which is usually formatted for its users, which makes it difficult to extract relevant data from various sources. Therefore, the availability of robust, flexible Information Extraction (IE) systems that transform the Web pages into program-friendly structures such as a relational database will become a great necessity. Although many approaches for data extraction from Web pages have been developed, there has been limited effort to compare such tools. Unfortunately, in only a few cases can the results generated by distinct tools be directly compared since the addressed extraction tasks are different. This paper surveys the major Web data extraction approaches and compares them in three dimensions: the task domain, the automation degree, and the techniques used. The criteria of the first dimension explain why an IE system fails to handle some Web sites of particular structures. The criteria of the second dimension classify IE systems based on the techniques used. The criteria of the third dimension measure the degree of automation for IE systems. We believe these criteria provide qualitatively measures to evaluate various IE approaches.},
author = {Chang, Chia-Hui and Kayed, Mohammed and Girgis, Moheb Ramzy and Shaalan, Khaled F},
doi = {10.1109/TKDE.2006.152},
file = {:Users/joaomateusdefreitasveneroso/Books/01683775.pdf:pdf},
isbn = {1041-4347 VO - 18},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Information extraction,Web mining,wrapper,wrapper induction},
number = {10},
pages = {1411--1428},
title = {{A Survey of Web Information Extraction Systems}},
url = {http://doi.ieeecomputersociety.org/10.1109/TKDE.2006.152},
volume = {18},
year = {2006}
}
@book{Ribas2015,
address = {New York, New York, USA},
author = {Ribas, Sabir and Ribeiro-Neto, Berthier and Santos, Rodrygo and {Souza e Silva}, Edmundo and Ueda, Alberto and Ziviani, Nivio},
booktitle = {the 2015 International Conference},
doi = {10.1145/2808194.2809462},
file = {:Users/joaomateusdefreitasveneroso/Books/ribas2015ictir-2.pdf:pdf},
isbn = {9781450338332},
keywords = {academic search,random walks,reputation flows},
pages = {181--190},
publisher = {ACM Press},
title = {{Random Walks on the Reputation Graph}},
url = {http://dl.acm.org/citation.cfm?doid=2808194.2809462 http://dl.acm.org/ft{\_}gateway.cfm?id=2809462{\&}type=pdf{\%}0Ahttp://dl.acm.org/citation.cfm?doid=2808194.2809462{\%}0Ahttp://dl.acm.org/citation.cfm?id=2808194.2809462{\%}0Ahttp://dl.acm.org/citation.cfm?id=2809462{\&}},
year = {2015}
}
@article{Liu2000,
abstract = {The paper describes the methodology and the software development of XWRAP, an XML-enabled wrapper construction system for semi-automatic generation of wrapper programs. By XML-enabled we mean that the metadata about information content that are implicit in the original Web pages will be extracted and encoded explicitly as XML tags in the wrapped documents. In addition, the query based content filtering process is performed against the XML documents. The XWRAP wrapper generation framework has three distinct features. First, it explicitly separates tasks of building wrappers that are specific to a Web source from the tasks that are repetitive for any source, and uses a component library to provide basic building blocks for wrapper programs. Second, it provides a user friendly interface program to allow wrapper developers to generate their wrapper code with a few mouse clicks. Third and most importantly, we introduce and develop a two-phase code generation framework. The first phase utilizes an interactive interface facility to encode the source-specific metadata knowledge identified by individual wrapper developers as declarative information extraction rules. The second phase combines the information extraction rules generated at the first phase with the XWRAP component library to construct an executable wrapper program for the given Web source. We report the initial experiments on performance of the XWRAP code generation system and the wrapper programs generated by XWRAP},
author = {Liu, L. and Pu, C. and Han, W.},
doi = {10.1109/ICDE.2000.839475},
file = {:Users/joaomateusdefreitasveneroso/Books/10.1.1.123.4425.pdf:pdf},
isbn = {0-7695-0506-6},
issn = {1063-6382},
journal = {Proceedings of 16th International Conference on Data Engineering},
pages = {611--621},
title = {{XWRAP: an XML-enabled wrapper construction system for Web information sources}},
url = {http://ieeexplore.ieee.org/document/839475/},
year = {2000}
}
@article{Hsu1998,
abstract = {Integrating a large number of Web information sources may significantly increase the utility of the World-Wide Web. A promising solution to the integration is through the use of a Web Information mediator that provides seamless, transparent access for the clients. Information mediators need wrappers to access a Web source as a structured database, but building wrappers by hand is impractical. Previous work on wrapper induction is too restrictive to handle a large number of Web pages that contain tuples with missing attributes, multiple values, variant attribute permutations, exceptions and typos. This paper presents SoftMealy, a novel wrapper representation formalism. This representation is based on a finite-state transducer (FST) and contextual rules. This approach can wrap a wide range of semistructured Web pages because FSTs can encode each different attribute permutation as a path. A SoftMealy wrapper can be induced from a handful of labeled examples using our generalization algorithm. We have implemented this approach into a prototype system and tested it on real Web pages. The performance statistics shows that the sizes of the induced wrappers as well as the required training effort are linear with regard to the structural variance of the test pages. Our experiment also shows that the induced wrappers can generalize over unseen pages.},
author = {Hsu, Chun Nan and Dung, Ming Tzung},
doi = {10.1016/S0306-4379(98)00027-1},
file = {:Users/joaomateusdefreitasveneroso/Books/8265afd280486299a5b8f1dbaaf6769422de.pdf:pdf},
isbn = {0306-4379},
issn = {03064379},
journal = {Information Systems},
keywords = {information extraction,semistructured data,world wide web,wrapper induction},
number = {8},
pages = {521--538},
title = {{Generating finite-state transducers for semi-structured data extraction from the Web}},
volume = {23},
year = {1998}
}
@article{Kushmerick2000,
abstract = {The Internet presents numerous sources of useful information - telephone directories, product catalogs, stock quotes, event listings, etc. Recently, many systems have been built that automatically gather and manipulate such information on a user's behalf. However, these resources are usually formatted for use by people (e.g., the relevant content is embedded in HTML pages), so extracting their content is difficult. Most systems use customized wrapper procedures to perform this extraction task. Unfortunately, writing wrappers is tedious and error-prone. As an alternative, we advocate wrapper induction, a technique for automatically constructing wrappers. In this article, we describe six wrapper classes, and use a combination of empirical and analytical techniques to evaluate the computational tradeoffs among them. We first consider expressiveness: how well the classes can handle actual Internet resources, and the extent to which wrappers in one class can mimic those in another. We then turn to efficiency: we measure the number of examples and time required to learn wrappers in each class, and we compare these results to PAC models of our task and asymptotic complexity analyses of our algorithms. Summarizing our results, we find that most of our wrapper classes are reasonably useful (70{\%} of surveyed sites can be handled in total), yet can rapidly learned (learning usually requires just a handful of examples and a fraction of a CPU second per example).},
annote = {wrappers induction is the process of generating wrappers manually.

there are six classes of wrappers.

wrappers are defined by delimiters},
author = {Kushmerick, Nicholas},
doi = {10.1016/S0004-3702(99)00100-9},
file = {:Users/joaomateusdefreitasveneroso/Books/kushmerick00wrapper.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {information extraction,internet information integration,machine learning,wrapper induction},
number = {1-2},
pages = {15--68},
title = {{Wrapper induction: efficiency and expressiveness}},
volume = {118},
year = {2000}
}
@article{Arocena1999,
abstract = {The widespread use of the Web has originated several new data$\backslash$nmanagement problems, such as extracting data from Web pages and making$\backslash$ndatabases accessible from Web browsers, and has renewed the interest in$\backslash$nproblems that had appeared before in other contexts, such as querying$\backslash$ngraphs, semistructured data and structured documents. Several systems$\backslash$nand languages have been proposed for solving each of these Web data$\backslash$nmanagement problems, but none of these systems addresses all the$\backslash$nproblems from a unified perspective. Many of these problems essentially$\backslash$namount to data restructuring: we have information represented according$\backslash$nto a certain structure and we want to construct another representation$\backslash$nof (part of it) using a different structure. We present the WebOQL$\backslash$nsystem, which supports a general class of data restructuring operations$\backslash$nin the context of the Web. WebOQL synthesizes ideas from query languages$\backslash$nfor the Web, for semistructured data and for Website restructuring},
author = {Arocena, Gustavo O. and Mendelzon, Alberto O.},
doi = {10.1002/(SICI)1096-9942(1999)5:3<127::AID-TAPO2>3.0.CO;2-X},
file = {:Users/joaomateusdefreitasveneroso/Books/10.1.1.34.9263.pdf:pdf},
isbn = {0-8186-8289-2},
issn = {10743227},
journal = {Theory and Practice of Object Systems},
number = {3},
pages = {127--141},
title = {{WebOQL: Restructuring documents, databases, and webs}},
volume = {5},
year = {1999}
}
@article{Soderland1999,
abstract = {A wealth of on-line text information can be made available to automatic processing by information extraction (IE) systems. Each IE application needs a separate set of rules tuned to the domain and writing style. WHISK helps to overcome this knowledge-engineering bottleneck by learning text extraction rules automatically.},
author = {Soderland, Stephen},
doi = {10.1023/A:1007562322031},
file = {:Users/joaomateusdefreitasveneroso/Books/10.1.1.41.8809.pdf:pdf},
isbn = {1595930345},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {information extraction,natural language processing,rule learning},
number = {1},
pages = {233--272},
title = {{Learning Information Extraction Rules for Semi-Structured and Free Text}},
url = {http://www.springerlink.com/content/m23n8197vg924t51/abstract/{\%}5Cnhttp://www.springerlink.com/content/m23n8197vg924t51/fulltext.pdf},
volume = {34},
year = {1999}
}
@article{Khalil2017,
abstract = {RCrawler is a contributed R package for domain-based web crawling and content scraping. As the first implementation of a parallel web crawler in the R environment, RCrawler can crawl, parse, store pages, extract contents, and produce data that can be directly employed for web content mining applications. However, it is also flexible, and could be adapted to other applications. The main features of RCrawler are multi-threaded crawling, content extraction, and duplicate content detection. In addition, it includes functionalities such as URL and content-type filtering, depth level controlling, and a robot.txt parser. Our crawler has a highly optimized system, and can download a large number of pages per second while being robust against certain crashes and spider traps. In this paper, we describe the design and functionality of RCrawler, and report on our experience of implementing it in an R environment, including different optimizations that handle the limitations of R. Finally, we discuss our experimental results.},
author = {Khalil, Salim and Fakir, Mohamed},
doi = {10.1016/j.softx.2017.04.004},
file = {:Users/joaomateusdefreitasveneroso/Books/1-s2.0-S2352711017300110-main.pdf:pdf},
issn = {23527110},
journal = {SoftwareX},
keywords = {Data collection,Parallel crawling,R package,Web crawler,Web mining,Web scraper},
pages = {98--106},
publisher = {Elsevier B.V.},
title = {{RCrawler: An R package for parallel web crawling and scraping}},
url = {http://dx.doi.org/10.1016/j.softx.2017.04.004},
volume = {6},
year = {2017}
}
@article{Ferrara2014,
abstract = {Web Data Extraction is an important problem that has been studied by means of different scientific tools and in a broad range of applications. Many approaches to extracting data from the Web have been designed to solve specific problems and operate in ad-hoc domains. Other approaches, instead, heavily reuse techniques and algorithms developed in the field of Information Extraction. This survey aims at providing a structured and comprehensive overview of the literature in the field of Web Data Extraction. We provided a simple classification framework in which existing Web Data Extraction applications are grouped into two main classes, namely applications at the Enterprise level and at the Social Web level. At the Enterprise level, Web Data Extraction techniques emerge as a key tool to perform data analysis in Business and Competitive Intelligence systems as well as for business process re-engineering. At the Social Web level, Web Data Extraction techniques allow to gather a large amount of structured data continuously generated and disseminated by Web 2.0, Social Media and Online Social Network users and this offers unprecedented opportunities to analyze human behavior at a very large scale. We discuss also the potential of cross-fertilization, i.e., on the possibility of re-using Web Data Extraction techniques originally designed to work in a given domain, in other domains.},
archivePrefix = {arXiv},
arxivId = {1207.0246},
author = {Ferrara, Emilio and {De Meo}, Pasquale and Fiumara, Giacomo and Baumgartner, Robert},
doi = {10.1016/j.knosys.2014.07.007},
eprint = {1207.0246},
file = {:Users/joaomateusdefreitasveneroso/Books/1-s2.0-S0950705114002640-main.pdf:pdf},
isbn = {978-1-4244-5678-9},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Business intelligence,Information retrieval,Knowledge engineering,Knowledge-based systems,Web data mining,Web information extraction},
pages = {301--323},
pmid = {18244404},
publisher = {Elsevier B.V.},
title = {{Web data extraction, applications and techniques: A survey}},
url = {http://dx.doi.org/10.1016/j.knosys.2014.07.007},
volume = {70},
year = {2014}
}
@inproceedings{Zhu2006,
 author = {Zhu, Jun and Nie, Zaiqing and Wen, Ji-Rong and Zhang, Bo and Ma, Wei-Ying},
 title = {Simultaneous Record Detection and Attribute Labeling in Web Data Extraction},
 booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '06},
 year = {2006},
 isbn = {1-59593-339-5},
 location = {Philadelphia, PA, USA},
 pages = {494--503},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1150402.1150457},
 doi = {10.1145/1150402.1150457},
 acmid = {1150457},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {attribute labeling, conditional random fields, data record detection, hierarchical conditional random fields, web page segmentation},
}
@inproceedings{Zhu2005,
 author = {Zhu, Jun and Nie, Zaiqing and Wen, Ji-Rong and Zhang, Bo and Ma, Wei-Ying},
 title = {2D Conditional Random Fields for Web Information Extraction},
 booktitle = {Proceedings of the 22Nd International Conference on Machine Learning},
 series = {ICML '05},
 year = {2005},
 isbn = {1-59593-180-5},
 location = {Bonn, Germany},
 pages = {1044--1051},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1102351.1102483},
 doi = {10.1145/1102351.1102483},
 acmid = {1102483},
 publisher = {ACM},
 address = {New York, NY, USA},
} 
@inproceedings{Furche2012a,
 author = {Furche, Tim and Gottlob, Georg and Grasso, Giovanni and Gunes, Omer and Guo, Xiaoanan and Kravchenko, Andrey and Orsi, Giorgio and Schallhart, Christian and Sellers, Andrew and Wang, Cheng},
 title = {DIADEM: Domain-centric, Intelligent, Automated Data Extraction Methodology},
 booktitle = {Proceedings of the 21st International Conference on World Wide Web},
 series = {WWW '12 Companion},
 year = {2012},
 isbn = {978-1-4503-1230-1},
 location = {Lyon, France},
 pages = {267--270},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2187980.2188025},
 doi = {10.1145/2187980.2188025},
 acmid = {2188025},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data extraction, deep web, knowledge},
} 
@article{Huang2015,
  author    = {Zhiheng Huang and
               Wei Xu and
               Kai Yu},
  title     = {Bidirectional {LSTM-CRF} Models for Sequence Tagging},
  journal   = {CoRR},
  volume    = {abs/1508.01991},
  year      = {2015},
  url       = {http://arxiv.org/abs/1508.01991},
  archivePrefix = {arXiv},
  eprint    = {1508.01991},
  timestamp = {Mon, 13 Aug 2018 16:46:46 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HuangXY15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Lample2016,
  author    = {Guillaume Lample and
               Miguel Ballesteros and
               Sandeep Subramanian and
               Kazuya Kawakami and
               Chris Dyer},
  title     = {Neural Architectures for Named Entity Recognition},
  journal   = {CoRR},
  volume    = {abs/1603.01360},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.01360},
  archivePrefix = {arXiv},
  eprint    = {1603.01360},
  timestamp = {Mon, 13 Aug 2018 16:47:39 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LampleBSKD16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Ma2016,
  author    = {Xuezhe Ma and
               Eduard H. Hovy},
  title     = {End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF},
  journal   = {CoRR},
  volume    = {abs/1603.01354},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.01354},
  archivePrefix = {arXiv},
  eprint    = {1603.01354},
  timestamp = {Mon, 13 Aug 2018 16:46:12 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MaH16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{Leek1997,
    author = {Timothy Robert Leek},
    title = {Information Extraction Using Hidden Markov Models},
    year = {1997}
}
@inproceedings{Freitag1999,
    author = {Dayne Freitag and Andrew Kachites Mccallum},
    title = {Information Extraction with HMMs and Shrinkage},
    booktitle = {In Proceedings of the AAAI-99 Workshop on Machine Learning for Information Extraction},
    year = {1999},
    pages = {31--36}
}
@inproceedings{Lafferty2001,
    author = {John Lafferty},
    title = {Conditional random fields: Probabilistic models for segmenting and labeling sequence data},
    year = {2001},
    pages = {282--289},
    publisher = {Morgan Kaufmann}
}
@inproceedings{McCallum2000,
 author = {McCallum, Andrew and Freitag, Dayne and Pereira, Fernando C. N.},
 title = {Maximum Entropy Markov Models for Information Extraction and Segmentation},
 booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
 series = {ICML '00},
 year = {2000},
 isbn = {1-55860-707-2},
 pages = {591--598},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=645529.658277},
 acmid = {658277},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 
@book{Li2000,
 author = {Li, Jia and Gray, Robert M.},
 title = {Image Segmentation and Compression Using Hidden Markov Models},
 year = {2000},
 isbn = {0792378997},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA},
} 
@incollection{Rabiner1990,
 author = {Rabiner, Lawrence R.},
 chapter = {A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition},
 title = {Readings in Speech Recognition},
 editor = {Waibel, Alex and Lee, Kai-Fu},
 year = {1990},
 isbn = {1-55860-124-4},
 pages = {267--296},
 numpages = {30},
 url = {http://dl.acm.org/citation.cfm?id=108235.108253},
 acmid = {108253},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 
@inproceedings{Freitag2000,
 author = {Freitag, Dayne and McCallum, Andrew},
 title = {Information Extraction with HMM Structures Learned by Stochastic Optimization},
 booktitle = {Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence},
 year = {2000},
 isbn = {0-262-51112-6},
 pages = {584--589},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=647288.723414},
 acmid = {723414},
 publisher = {AAAI Press},
} 
@article{Liu1989,
 author="Liu, Dong C.
 and Nocedal, Jorge",
 title="On the limited memory BFGS method for large scale optimization",
 journal="Mathematical Programming",
 year="1989",
 month="Aug",
 day="01",
 volume="45",
 number="1",
 pages="503--528",
}
@article{Hochreiter1997,
 author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
 title = {Long Short-Term Memory},
 journal = {Neural Comput.},
 issue_date = {November 15, 1997},
 volume = {9},
 number = {8},
 month = nov,
 year = {1997},
 issn = {0899-7667},
 pages = {1735--1780},
 numpages = {46},
 url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
 doi = {10.1162/neco.1997.9.8.1735},
 acmid = {1246450},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 
@inproceedings{Sang2003,
 author = {Tjong Kim Sang, Erik F. and De Meulder, Fien},
 title = {Introduction to the CoNLL-2003 Shared Task: Language-independent Named Entity Recognition},
 booktitle = {Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 - Volume 4},
 series = {CONLL '03},
 year = {2003},
 location = {Edmonton, Canada},
 pages = {142--147},
 numpages = {6},
 url = {https://doi.org/10.3115/1119176.1119195},
 doi = {10.3115/1119176.1119195},
 acmid = {1119195},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 
@inproceedings{Caruana2000,
 author = {Caruana, Rich and Lawrence, Steve and Giles, Lee},
 title = {Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping},
 booktitle = {Proceedings of the 13th International Conference on Neural Information Processing Systems},
 series = {NIPS'00},
 year = {2000},
 location = {Denver, CO},
 pages = {381--387},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=3008751.3008807},
 acmid = {3008807},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}
@inproceedings{Pennington2014,
    author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
    title = {Glove: Global vectors for word representation},
    booktitle = {In EMNLP},
    year = {2014}
}
@Inbook{Ramshaw1999,
author="Ramshaw, L. A.
and Marcus, M. P.",
editor="Armstrong, Susan
and Church, Kenneth
and Isabelle, Pierre
and Manzi, Sandra
and Tzoukermann, Evelyne
and Yarowsky, David",
title="Text Chunking Using Transformation-Based Learning",
bookTitle="Natural Language Processing Using Very Large Corpora",
year="1999",
publisher="Springer Netherlands",
address="Dordrecht",
pages="157--176",
abstract="Transformation-based learning, a technique introduced by Eric Brill (1993b), has been shown to do part-of-speech tagging with fairly high accuracy. This same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive ``baseNP'' chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 93{\%} for baseNP chunks (trained on 950K words) and 88{\%} for somewhat more complex chunks that partition the sentence (trained on 200K words). Working in this new application and with larger template and training sets has also required some interesting adaptations to the transformation-based learning approach.",
isbn="978-94-017-2390-9",
doi="10.1007/978-94-017-2390-9_10",
url="https://doi.org/10.1007/978-94-017-2390-9_10"
}
@book{Rijsbergen1979,
 author = {Rijsbergen, C. J. Van},
 title = {Information Retrieval},
 year = {1979},
 isbn = {0408709294},
 edition = {2nd},
 publisher = {Butterworth-Heinemann},
 address = {Newton, MA, USA},
} 
[download]
