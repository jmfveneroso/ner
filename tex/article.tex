\documentclass[sigconf]{acmart}
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers
\setcopyright{none}

\usepackage{booktabs} % For formal tables
\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{multirow}

\DeclareMathOperator*{\argmax}{arg\,max\,}
\renewcommand{\arraystretch}{1.2}

\begin{document}

\title{Named Entity Recognition on HTML as a web data extraction subtask}

\author{João Mateus de Freitas Veneroso}
\affiliation{%
  \institution{Universidade Federal de Minas Gerais}
  \streetaddress{Av. Pres. Antônio Carlos, 6627 - Pampulha}
  \city{Belo Horizonte}
  \state{MG}
  \postcode{31270-901}
  \country{Brazil}}
\email{jmfveneroso@gmail.com}

\author{Berthier Ribeiro-Neto}
\affiliation{%
  \institution{Universidade Federal de Minas Gerais}
  \streetaddress{Av. Pres. Antônio Carlos, 6627 - Pampulha}
  \city{Belo Horizonte}
  \state{MG}
  \postcode{31270-901}
  \country{Brazil}}
\email{berthier@dcc.ufmg.br}
\email{berthier@google.com}

\begin{abstract}

Web data extraction (WDE) methods often rely on hand-coded rules to 
identify and extract data from web pages. These methods 
may be suited to extract information from single websites, but
they usually perform poorly on extraction tasks across different 
websites or different application domains. Statistical methods
are more flexible in this regard. In this paper, we investigate
different approaches to Named Entity Recognition (NER) on HTML, an
essential substask in statistical models of web data extraction.
Webpages can be very different from plain text due to their 
semistructured nature, so we introduced a novel dataset that focuses
on researcher name extraction from faculty listings from numerous
university websites to evaluate different models in the 
NER on HTML task. We found that neural network
architectures with a Conditional Random Fields output layer and
neural character representations are able to achieve an F1-score
of 0.8867 with no inputs besides word embeddings. These NER models 
can be part of a flexible and effective approach to WDE that does not
rely on gazetteers, regular expressions or any hand-coded rules. 

\end{abstract}

% \keywords{Named entity recognition, information extraction, web data extraction}

\maketitle

\section{Introduction}

Web data extraction (WDE) is the task of automatically extracting structured information
from unstructured or semi-structured web documents. The input usually consists of
web documents containing a number of predetermined entities organized in a similar 
manner and the information extraction task consists of identifying these entities
and organizing them according to a template. In this context, named entity recognition
(NER) is a subtask that aims to detect named entities in the text and classify them into 
predetermined categories such as person names, locations or organizations. 

HTML documents most often lie in between the structured / unstructured data paradigm.
DOM hierarchy, element disposition, class names, and other features related to 
the document structure and indirectly associated with the data itself can be valuable 
information on the task of identifying entities and determining relationships. Yet we 
cannot expect these features to be completely constrained by an underlying pattern. 
Organization patterns tend to follow some guidelines but are in no way subject to 
strict rules. That is why classical information extraction systems such as automatic
wrapper generators usually do not translate well across different websites. 

Most existing web data extraction methods are tailored to extract data from a single
web page \cite{Crescenzi2001, Arasu2003, Freitag1998, Califf1999, Soderland1999,
Kushmerick2000, Hsu1998, Muslea1999}, producing different compromises between efficacy
and degree of human supervision. More recent unsupervised approaches proposed to
tackle the problem of data extraction for whole application domains \cite{Zhu2005, Zhu2006, 
Abdessalem2010, Furche2012, Furche2012a}. Unsupervised WDE methods usually operate in at
least two steps: record segmentation and attribute labelling. Record segmentation
is essentially clustering visually and structurally similar web page regions and 
identifying repeating data records. Attribute labelling is the task of identifying
attributes in data records. Doing any of these tasks well can help performing the 
other task, since patterns that are present in some data records can help identifying 
attributes in other data records. And by properly identifying attributes, we are better 
able to determine the boundaries of data records and segment them with increased accuracy.

In recent years, we saw an astonishing progress in Natural Language Processing.
However, despite WDE being a closely related field of research,
many tools rely on heuristics or hand-coded rules to perform record segmentation,
and many methods resort to regular expressions or gazetteer matching to perform attribute
labelling. While these approaches can be sufficient to extract information from webpages
with similar templates and identifying attributes with distinct formats like prices and
dates, we still lack a sufficiently flexible and intelligent approach to handle more complicated cases.
Even when extracting data from web documents only, data records may contain pure text with 
relevant named entities. Also, many WDE algorithms can effectively exploit the semi-
structure of web documents, but when a method relies too much on structural features, it
often produces poor generalization on cross website extraction tasks. Contrastingly, we 
understand that statistical based approaches can be much more flexible.

In this paper, we investigate the attribute labelling task in WDE with a named entity 
recognition framework. Recently proposed neural architectures have achieved exciting results 
on the NER task on plain text \cite{Huang2015, Lample2016, Ma2016} while requiring almost 
no feature engineering or gazetteers. NER on HTML poses a different type of challenge, 
because web pages are often very different from plain text. Named entities may be present 
inside tables, lists, graphs or other types of visual elements that provide
little to no textual information that could give hints about the semantic category of 
a word. However we show that we can get exceptional performance even with this limitation
and small training sets.

By reliably detecting named entities, we can improve the performance of existing WDE 
approaches or even construct a full machine learning based tool to tackle domain wide
extraction with great flexibility. To test different NER approaches from the perspective 
of web data extraction,
we explored the task of person names extraction from university faculty listings.
Researcher affiliation is often missing from many entries in public databases 
and the display of information varies significantly between different university 
websites, so this task can provide a good measure of the expected performance and
data need for other web data extraction tasks. 

\section{Related Work}

In the last 20 years, the astonishing growth of public information in the web has 
led to the development of a number of different approaches to the problem of web 
information extraction. Traditionally, the task was solved by designing special purpose
programs called wrappers to recognize relevant data and store records in a structured
format. These early tools varied wildly relative to their degree of automation. 

It was readily perceived that manual wrapper generation was a rather tedious and
error prone process, unsuited for large scale operations. Wrappers tend to
break frequently because they rely heavily on web page features that can change 
often. So, in the late nineties, several authors advocated for wrapper induction, a technique 
that consists of automatically constructing wrappers from a small set of examples by 
identifying delimiters or context tokens that single out the desired attributes. 
Some remarkable wrapper induction methods are WIEN \cite{Kushmerick2000}, Soft 
Mealy \cite{Hsu1998} and STALKER \cite{Muslea1999}.

Despite being better than constructing wrappers manually, wrapper induction methods 
still suffered from a lack of expressive power and flexibility. These methods had 
trouble handling records with missing attributes or unusual structures because
patterns could only be identified if they happened at least once in the examples.

Other approaches such as NoDoSE \cite{Adelberg1998} and Debye \cite{Laender2002a} 
brought greater flexibility to wrapper induction methods by requiring a greater level 
of human interaction through graphical user interfaces. Web data extraction techniques often 
require some sort of assistance from human experts to boost accuracy. One of the main challenges 
in the field lies in determining an adequate tradeoff between the degree of automation and 
the precision and recall of the data extraction tool.

To automate the task of web data extraction completely some approaches,
such as Road Runner \cite{Crescenzi2001}, removed entirely the need for data examples.
Road Runner parses documents belonging to a same class (e.g. books on Amazon) and 
generates wrappers based on their similarities and differences, yielding comparable results 
to those obtained by wrapper induction methods. However like previous approaches, it was 
unsuited for cross site extraction tasks because the learned rules were not general enough.

NLP based approaches aimed at extracting more general rules that could possibly
be employed over multiple websites. RAPIER \cite{Califf1999} is a method of rule
extraction that uses information such as part-of-speech tags and semantic classes from
a lexicon to derive patterns from a set of training examples. This approach is more
flexible than the wrapper induction methods, however it achieves much lower rates of 
recall and precision.

In 2002, a survey by Laender et al. \cite{Laender2002} made a thorough classification of the
early approaches with a taxonomy based on their main technology, being them: languages for
wrapper development, HTML-aware tools, NLP-based tools, Wrapper Induction Tools,
Modeling-based tools and Ontology-based tools. Some noteworthy examples from this era
are: 

\begin{itemize}
\item TSIMMIS \cite{Hammer1997} and WebOQL \cite{Arocena1999}, which are special purpose 
languages for building wrappers.

\item Road Runner \cite{Crescenzi2001}, XWRAP \cite{Liu2000} and W4F \cite{Sahuguet1999}, 
which are HTML-aware tools that infer meaningful patterns from the HTML structure.

\item RAPIER \cite{Califf1999}, SRV \cite{Freitag1998}, WHISK \cite{Soderland1999}, which 
are NLP-based tools.

\item WIEN \cite{Kushmerick2000}, Soft Mealy \cite{Hsu1998} and STALKER \cite{Muslea1999} which 
are wrapper induction methods.

\item NoDoSE \cite{Adelberg1998} and Debye \cite{Laender2002a}, which are semi supervised modeling
based tools that require some interaction with the user by means of a graphical
user interface.
\end{itemize}

In 2006, Chang et al. \cite{Chang2006} complemented the previous surveys with semisupervised 
technologies such as Thresher \cite{Hogue2005}, IEPAD \cite{Chang2001} and 
OLERA \cite{Chang2004}. They differed from supervised 
and unsupervised methods because they either needed only a rough description of
data from users for extraction rule generation or some level of post processing
that needed user attention. The survey also mentioned newer unsupervised methods
such as DeLa \cite{Wang2003}, Exalg \cite{Arasu2003} and Depta \cite{Zhai2005}.

Most of the early information extraction systems were rule-based with either 
manual rule description or automatic rule learning from examples, thus they
suffered from a lack of flexibility when dealing with noisy and unstructured data.
Huge progress in the field of statistical learning led to the development of
statistical models that tried to solve this problem.

In 2008, Sarawagi \cite{Sarawagi2008} produced a survey that classified wrappers into
rule-based methods, statistical methods and hybrid models, bringing together 
the fields of named entity recognition, relationship extraction and information extraction. 
The rule based methods encompass most of the 
previous models. The statistical methods convert the extraction task into a token labeling 
task, identifying the target entities through the assignment of labels as in a typical 
Named Entity Recognition task. Traditionally, Hidden Markov Models \cite{Leek1997, Freitag1999}, 
Linear Chain Conditional Random Fields \cite{Lafferty2001}, and Maximum Entropy Taggers 
\cite{McCallum2000} have been the usual choice for linear sequence tagging models.
More recently, with the advancement of Natural Language Processing and Deep Learning, 
neural models outperformed previous NER methods. Huang et. al. \cite{Huang2015} introduced the 
bidirectional Long Short-Term Memory (LSTM) model with a Conditional Random Field (CRF) output layer
for NER. Lample et. al. \cite{Lample2016} incorporated Convolutional Neural Network based character representations 
on top of the architecture. And Ma and Hovy \cite{Ma2016} introduced
LSTM based character representations. Both obtaining improvements with systems that are less reliant
on feature engineering.

Surveys by Ferrara et al. \cite{Ferrara2014}, Schulz et al. \cite{Schulz2016} and 
Varlamov et al. \cite{Varlamov2016} updated the previous surveys on information 
extraction methods with some interesting innovations. 
Some examples are: the Visual Box Model \cite{Krupl2005}, a data extraction system that produces 
a visualization of the web page to exploit visual cues to identify data presented in a tabular form;
automatic wrapper adaptation \cite{Ferrara2011}, a technique that tries to reduce the cost of 
wrapper maintenance by measuring the similarity of HTML trees and adapting
wrappers to the new page structure; AutoRM \cite{Shi2015}, a method to mine
records from a single web page by identifying similar data regions through DOM
tree analysis; Knowledge Vault \cite{Dong2014}, a method that combines different 
extraction approaches to feed a probabilistic knowledge base.

Most data extraction systems focus on extracting information from single websites
and are therefore unsuited for cross website extraction tasks. Even unsupervised
approaches that are domain independent, such as RoadRunner \cite{Crescenzi2001} 
and EXALG \cite{Arasu2003} only work well for extracting data from pages generated 
from the same template. A statistical approach to unsupervised domain 
independent web data extraction was described by Zhu et al \cite{Zhu2005}. The 2D CRF 
model takes a web page segmented into data blocks and employs a two dimensional conditional 
random field model to perform attribute labelling. The model was further improved in 
\cite{Zhu2006} to model record segmentation and attribute labelling as a joint task.
Some of the limitations of early unsupervised methods 
were also tackled by ObjectRunner \cite{Abdessalem2010} and AMBER \cite{Furche2012}. 
These methods work by annotating webpages automatically with regular expressions, gazetteers and 
knowledge bases. They can rectify low quality annotations and even improve the annotators
by exploring regular structures in the DOM during the record segmentation phase.

Web data extraction methods have undoubtedly improved extraordinarily, but
as pointed by Schulz et al. \cite{Schulz2016}, it is difficult to compare the results 
achieved by competing tools, and many seem to rely excessively on certain heuristics.
In that regard, the impressive advancements in NLP may provide for more robust and
flexible extraction tools.

\section{Named Entity Recognition Models}

Most web data extraction systems rely on hand crafted rules or gazetteers to perform
attribute annotation. Machine learning approaches to NER can improve annotations of 
more complex entities and even manage entity detection without resorting to a gazetteer.
We explored multiple approaches to the Named Entity Recognition problem in the context 
of a web information extraction task. First, we introduce two traditional approaches, 
namely: Hidden Markov Models and Linear Chain Conditional Random Fields. Then
we explore neural network architectures.

\subsection{Hidden Markov Models}

A Markov Model is a stochastic model that computes the most probable sequence of states 
given a limited set of observable states $ S = \{s_1, s_2, ..., s_n \} $.
The Hidden Markov Model (HMM) differs from the Markov Model in that it
does not observe the states directly, but rather a probabilistic function of those 
states. For example in NER, the words are observed, however the Named Entity labels
associated with these words are not. Formally, we want to compute the most probable
sequence of labels $ Y = \{y_1, y_2, ..., y_n\} $ for a sequence of observed tokens
$ X = \{x_1, x_2, ..., x_n\} $.

\begin{equation}
Y^* = \argmax_{Y} P(Y|X)
\end{equation}

With Bayes theorem, we can write $ P(Y|X) $ as:

\begin{equation}
P(Y|X) = \frac{P(X|Y) P(Y)}{P(X)}
\end{equation}

Since $ P(X) $ is the same for all label sequences $ Y $, we can simply maximize
$ P(X|Y) P(Y) $.

A HMM makes two assumptions. First, the probability of being in a given state depends 
only on a fixed number of previous states. That is 
$ P(y_i|y_{i-1}x_{i-1}, y_{i-2}x_{i-2}, ..., y_1x_1) = P(y_i|y_{i-1}, y_{i-2},..., y_{i-k}) $. 
In fact, we can get much better results on the NER task by looking at trigrams
or quadrigrams ($ k = 2 $ or $ k = 3 $) instead of bigrams as a regular HMM. 
Some label assignments are highly improbable, such as single token named entities 
separated by a common word, and these kinds of patterns can be caught by a higher order HMM.
Second, the probability of a word depends only on its assigned label 
$ P(x_i|y_{i-1}x_{i-1}, ..., y_1x_1) = P(x_i|y_i) $. With these assumptions, 
the probability $ P(Y|X) $ can be approximated by the expression:

\begin{equation}
P(Y|X) \propto \prod_{i=k+1}^{n} P(y_i|y_{i-1}, y_{i-2}, ..., y_{i-k}) P(x_i|y_i)
\end{equation}

The probabilities can be calculated through maximum likelihood estimation from the relative
frequencies of labels and features in the corpus. The best sequence of labels can be computed 
with a variable state Viterbi approach \cite{Li2000}. However, as we increase $ k $, this computation 
becomes exponentially more expensive. The beam-search strategy may be employed for a faster 
search, but we found that for $ k \leq 4 $, the Viterbi algorithm is still viable.

HMM based taggers have been succesfully apllied in many NLP and WDE tasks 
\cite{Rabiner1990, Leek1997, Freitag2000}. They are incredibly fast to train and very 
interpretable, making them a good choice for a first approximation. However, these models 
are highly dependent on the right selection of features, what may outweigh the benefit of a 
small training cost.

\subsubsection{Self training} \label{sssec:self_training}

In the particular case of NER on HTML, there are useful features related to the HTML
structure that can be used to identify named entities. In a particular website, named
entities tend to occur inside the same HTML tags with the same CSS classes. This fact
can be used as an additional piece of evidence in the HMM tagger. However, if we train 
the HMM for a particular set of websites, and then we feed it with new websites, almost 
surely there will be small correlation between the HTML features estimated in the training 
set and the distribution of these features in the new websites, simply because websites 
use very different templates. Therefore, we propose to use a self training strategy to train 
these HTML features. This is only possible because the HMM can be trained very quickly. The 
self training strategy is implemented like this:

\begin{itemize}
\item Train the HMM with the training set without HTML features.
\item Compute labels for the new websites with the trained HMM.
\item Use these computed labels as a proxy for the actual labels in the 
new websites and estimate HTML feature frequencies only in the new websites.
\item Recompute the labels using the HTML features.
\end{itemize}

This adds very little overhead to the original model while improving precision and recall
modestly.

\subsection{Linear Chain Conditional Random Fields}

A Linear Chain Conditional Random Field (CRF) is the discriminative analog to the HMM,
it was first introduced by Lafferty \cite{Lafferty2001}. It is a distribution $ P(Y|X) $ that takes the form:

\begin{equation}
P(Y|X) = \frac{1}{Z(x)} \prod_{t=1}^{T} exp \left( \sum_{k=1}^{K} \theta_k f_k(y_{t-1}, y_t, X) \right)
\end{equation}
\\

where $ \theta $ is the parameter vector that we are going to learn, $ f_k(y_{t-1}, y_{t}, X) $ 
are feature functions over the current timestep $ t_y $, the previous timestep $ y_{t-1}$, 
and the observation vector $ X $. And the partition function $ Z(x) $, takes the form:

\begin{equation}
Z(x) = \sum_{Y} \prod_{t=1}^{T} exp \left( \sum_{k=1}^{K} \theta_k f_k(y_{t-1}, y_t, x) \right)
\end{equation}
\\

which is a sum over all possible label assignments $ Y $. The partition function can be efficiently
and exactly calculated with the sum-product algorithm. Parameter estimation is usually done through 
negative log likelihood minimization. The function can be optimized with techniques suitable for other 
maximum entropy models such as L-BFGS \cite{Liu1989}. The most likely label sequences can be decoded 
with the Viterbi algorithm, as was the case on HMMs.

CRFs are more general than HMMs because the transitions from $ y_{t-1} $ to $ y_{t} $ can also depend 
on the vector of observations $ X $. This flexibility of feature functions allow for a wide range of
possibilities. Recently, CRFs have been succesfully employed as the output layer in complex neural 
architectures bringing improvements over models that classify labels independently.

\subsection{Neural Network Models}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{pics/rnn_network}
  \caption{RNN for NER}
  \label{fig:rnn_network}
\end{figure}

Recurrent neural networks (RNN) have been succesfully employed on numerous NLP tasks such as
language modelling, POS tagging, speech recognition and NER. Different from feedforward neural networks, 
RNNs can retain information in their internal state, making them more suitable for processing sequences, 
and consequently for solving text related tasks. In NER, the network outputs one label for each input 
vector as shown in Figure \ref{fig:rnn_network}. 

Although RNNs are theoretically capable of learning long term dependencies, in practice 
it might be difficult. Long short term memory networks (LSTM) were introduced by Hochreiter 
and Schmidhuber \cite{Hochreiter1997} with this problem in mind and have been 
popularized since them. LSTM cell implementations vary slightly in the literature.
Our implementation has the following definition:

\begin{flalign*}
\Gamma_i^{<t>} &= \sigma(W_i[x^{<t>},h^{<t-1>}] + b_i) &\\
\Gamma_f^{<t>} &= \sigma(W_f[x^{<t>},h^{<t-1>}] + b_f) &\\ 
c^{<t>}        &= \Gamma_f^{<t>} \circ c^{<t-1>} + \Gamma_i^{<t>} tanh(W_c[x^{<t>},h^{<t-1>}] + b_c) &\\
\Gamma_o^{<t>} &= \sigma(W_o[x^{<t>},h^{<t-1>}] + b_o) &\\
h^{<t>}        &= \Gamma_o^{<t>} \circ tanh(c^{t}) &
\end{flalign*}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{pics/lstm_cell}
  \caption{LSTM Cell}
  \label{fig:lstm_cell}
\end{figure}

where $ \sigma $ is the logistic sigmoid function. $ \Gamma_i $, $ \Gamma_f $, and $ \Gamma_o $ are the input,
forget and output gates, respectively. And $ W $ with a subscript is the weight 
matrix corresponding to that gate. Variable $ c^{<t>} $ is the cell 
state and $ h^{<t>} $ is the hidden state, with the vector $ [x^{<t>},h^{<t-1>}] $ 
being formed by concatenating the current input vector $ x^{<t>} $ and the hidden vector 
from the previous timestep $ h^{<t-1>} $. A visual description of our LSTM cell is 
provided in figure \ref{fig:lstm_cell}.
This implementation differs from the LSTM cell described in Huang et al. \cite{Huang2015}
in that the input gate $ \Gamma_i $ and the forget gate $ \Gamma_f $
do not take input from the previous cell state $ c^{<t-1>} $ and the output gate does
not take input from the current cell state $ c^{<t>} $. The cell state and the control gates 
are what makes the LSTM cell better at modelling long term dependencies in comparison
to a regular RNN.

In NER related tasks, both past and future words are important when deciding the
label at time $ t $, but a regular LSTM network only takes past features in consideration. 
A bidirectional LSTM solves this problem by stacking two regular LSTMs, and feeding
them with observations in opposite directions. The first LSTM receives the forward
states and the second LSTM receives the backward states. The hidden states from both 
networks can be concatenated at each time step to produce the output label. With this 
architecture, Bidirectional LSTMs have information from all input features in the sequence at 
any given timestep.

\subsubsection{LSTM-CRF}
\label{sssec:lstm_crf}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{pics/bi_lstm_crf}
  \caption{Bidirectional LSTM-CRF}
  \label{fig:bi_lstm_crf}
\end{figure}

Huang et al. \cite{Huang2015} proposed a bidirectional LSTM with a CRF layer (LSTM-CRF) on the output to tackle
the sequence tagging problem. The CRF layer jointly decodes labels for the whole sentence instead
of predicting each label individually. This architecture achieved an F1 score of 90.10 on the English
data from the ConLL-2003 NER shared task \cite{Sang2003}, in contrast to 85.17 for a bidirectional LSTM without the CRF layer. 
In our experiments, the LSTM-CRF architecture uses a bidirectional LSTM with 100 
hidden states, no peepholes and input and output dropout layers with a dropout
rate of 0.5. The dropout layers have proven to be very important to prevent overfitting 
and allow better generalization.

\subsubsection{LSTM-CRF + CNN character representations}
\label{sssec:lstm_crf_cnn}

\begin{figure}
  \centering
	  \includegraphics[width=0.5\textwidth]{pics/cnn}
  \caption{CNN based character representations}
  \label{fig:cnn}
\end{figure}

Ma and Hovy \cite{Ma2016} proposed adding a convolutional neural network (CNN) layer 
on top of a bidirectional LSTM-CRF to encode character-level information. The CNN
layer is described visually in Figure \ref{fig:cnn}, it receives
only character embeddings as inputs. The character representations generated by the CNN 
are combined with word level representations and then fed to the Bidirectional LSTM with
a CRF output layer described in section \ref{sssec:lstm_crf}.
This architecture can learn morphological features that are very
useful in the NER task, since similar named entities often present morphological similarities. 
This architecture obtained an F1 score of 91.21 in the ConLL2003 dataset. In our experiments, 
the LSTM-CRF architecture with CNN character representations uses a one dimensional convolutional 
neural network with 30 filters and a window size of three characters on top of the LSTM-CRF architecture.
The character embeddings fed to the CNN have 30 dimensions that are randomly initialized.

\subsubsection{LSTM-CRF + LSTM character representations}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{pics/lstm_char_representations}
  \caption{LSTM based character representations}
  \label{fig:lstm_char}
\end{figure}

Lample et al \cite{Lample2016} suggested using an a bidirectional LSTM to model character-level 
representations on a bidirectional LSTM-CRF. Combining both the forward and backward 
LSTM representations to form the character representation, as described in figure \ref{fig:lstm_char}. 
This character representation is also combined with a word representation and fed to a LSTM-CRF network. 
The forward LSTM is expected to be a better
representation of the suffix of a token, and the backward LSTM is expected to be a
better representation of the prefix of a token. This differentiates the architecture
from the CNN based approach described in Section \ref{sssec:lstm_crf_cnn}, because CNN filters 
discover positional invariant features, while LSTMs can better represent suffixes and prefixes. In our experiments, 
the LSTM-CRF architecture with LSTM character representations was implemented with a bidirectional LSTM
with 25 hidden states, producing character representations of size 50. The character embeddings
have 30 dimensions that are randomly initialized.

\subsubsection{Network training}

The neural models were trained with mini batch Stochastic Gradient Descent over 50 epochs with batch size 10,
learning rate 0.01, momentum 0.9 and decay rate 0.05. We used early stopping \cite{Caruana2000} to select the best 
parameters, considering the F1 measure in the validation set. All neural models used 
GloVe 100-dimensional word embeddings \cite{Pennington2014} that were fine tuned during training,
they have a vocabulary size of 400.000 words.
In the case of NER on HTML, word embeddings work similar to a gazetteer. Named entities 
with the same type have similar embeddings, so good word embeddings can achieve exceptional 
performance with little training and without a gazetteer. 

\section{NER on HTML Dataset}
\label{sec:ner_dataset}

We constructed a novel dataset to evaluate the performance of multiple NER models
at the web information extraction task. The task consists of finding researcher
names in faculty listings from multiple web pages. This would be the 
first step in linking researcher profiles from university websites to their entries
in public databases such as DBLP \footnote{http://dblp.uni-trier.de/}. Unlike many
information extraction datasets, each web page in the dataset comes from a different 
website, and therefore has a different format, what makes many information
extraction approaches impractical. The idea is to explore systems that are general 
enough to allow efficient name extraction from many different sources while requiring
no supervision between different websites. 

This task would be similar to the task of labeling authors in comments
from many different sources or labeling authors in articles collected from many
publishing platforms. Another similar task would be NER on tweets. Because of the character
limitation present on tweets, we end up with constraints comparable to the ones
found in the NER on HTML task, so we expect similar systems to have comparable performances
on both of these tasks.

We collected 145 computer science faculty pages from 42 different countries in
multiple languages, although the English version was prefered when it was available.
We gathered faculty web pages randomly in proportion to
the number of universities in each country\footnote{A detailed list of universities can
be found in https://univ.cc/world.php}. Each HTML page was preprocessed and converted
to the CONLL 2003 data format. That is, one word per line with empty lines representing
sentence boundaries. Sentence boundaries were determined by line break HTML tags
(div, p, table, li, br, etc.) in contrast to inline tags (span, em, a, td, etc.). 
Sentences that were more than fifty tokens long were also split according to punctuation.

A proper HTML segmenter poses many challenges by itself.
However, we wanted to evaluate models without relying on any sophisticated data record
segmentation system. In many cases, entity annotation may precede the segmentation
phase in web data extraction methods, so annotators should be able to work with
raw HTML data. Because of the type of documents present in the dataset, many sentences 
contained only one or two tokens, while other documents contained long texts.
Finally, all tokens were tagged using the IOB scheme put forward by
Ramshaw and Marcus \cite{Ramshaw1999}. 

\subsection{Data}

\begin{table}[h]
  \small
  \begin{center}
    \begin{tabular}{ lllll }
      \toprule
      Data file & Documents & Sentences & Tokens & Names \\
      \midrule
      Training    & 85 & 24728 & 110269 & 5822 \\  
      Validation  & 30 & 8743  & 36757  & 1788 \\
      Test        & 30 & 10399 & 44795  & 2708 \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{Number of HTML pages, sentences and tokens in each data file}
  \label{tab:dataset}
\end{table}

The dataset was divided in a training, validation and test set. Table \ref{tab:dataset} contains
a description of the data files. The validation set was used in the early stopping validation strategy
for the neural networks and CRF training, while the performance was evaluated by comparing results in the test set.

\subsection{Features}

Thirteen categorical features were also associated with each token in the dataset. They 
are presented in Table \ref{tab:features}.

\begin{table}[h]
  \small
  \begin{center}
    \begin{tabular}{ ll }
      \toprule
      Feature & Description \\
      \midrule
      1  & Unaccented lowercase token \\
      2  & Exact gazetteer match \\
      3  & Partial gazetteer match \\
      4  & Log name gazetteer count\\
      5  & Log word gazetteer count\\
      6  & Email \\
      7  & Number \\
      8  & Honorific \\
      9  & URL \\
      10 & Is capitalized \\
      11 & Is a punctuation sign \\
      12 & HTML tag + parent \\
      13 & CSS class \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{Features used in the NER on HTML dataset}
  \label{tab:features}
\end{table}

\begin{table}[h]
  \small
  \begin{center}
    \begin{tabular}{ lllll }
      \toprule
      Data file & Precision & Recall & F1 & Correct names \\
      \midrule
      Training   & 0.7316 & 0.2303 & 0.3504 & 1341 of 5822 \\ 
      Validation & 0.8474 & 0.2858 & 0.4274 & 511 of 1788 \\ 
      Test       & 0.8717 & 0.3287 & 0.4773 & 890 of 2708 \\ 
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{Gazetteer coverage in each data file}
  \label{tab:gazetteer}
\end{table}

The unaccented lowercase token was used as the key for the Glove-100 embedding lookup.
A gazetteer was constructed from a researcher list extracted from DBLP with 1.595.771
names. Table \ref{tab:gazetteer} shows the precision, recall and F1 score obtained with an
exact gazetteer matching strategy in each data file as a baseline.
Feature 2 represents an exact match of a sequence of tokens to any of the 1.595.771 
names, and feature 3 represents a partial match. Feature 4 is the rounded logarithm of 
the frequency of a token in the gazetteer, and feature 5 is the rounded logarithm of the frequency
of a token in a word corpus obtained through a random crawl in university websites.
Features 6 to 11 represent a simple regular expression match to an email, number, 
honorific, URL, capitalization and punctuation sign.

Feature 12 represents the HTML enclosing tag and its parent concatenated. Feature 13
represents all CSS classes concatenated. These features are not very useful in a general
sense, because every HTML document has a different format, so only because a named entity
occurs inside a given HTML tag in a document we cannot say it is more likely to be the case 
in other documents. However, these features can be trained with the self-training strategy 
for HMMs described in section \ref{sssec:self_training}. In all other models these features 
were ignored.

\section{Experiments}

Two experiments were conducted to evaluate the best models for named entity recognition
on HTML in the context of web data extraction. In the first experiment, we trained the NER models
using the entire training set (85 documents). In the second experiment, the training set was 
split into five non overlapping sets with 17 documents, and every model was trained
and tested once using each set, while the test data was kept unchanged. We wanted to verify empirically 
how the models behaved when training data is limited, since labelling NER datasets can be a
very time consuming effort.

Evaluation of model performance in this task was performend with the precision, recall and 
F1 score \cite{Rijsbergen1979}. Precision is the percentage of named entities found by 
the model that are correct. Recall is the percentage of named entities that are present
in the corpus and were found by the model. The F1 score is a composite measure that combines
precision and recall with the formula:

\begin{equation}
F1 = \frac{2 * precision * recall}{precision + recall}
\end{equation}

\begin{table}[h]
  \small
  \begin{center}
    \begin{tabular}{ ll }
      \toprule
      Model & Description \\
      \midrule
      hmm-1         & Regular HMM \\
      hmm-2         & HMM with $ k=2 $ \\
      hmm-3         & HMM with $ k=3 $ \\
      crf           & Linear chain conditional random fields \\
      lstm-crf      & LSTM-CRF model \cite{Huang2015} \\
      lstm-crf-cnn  & LSTM-CRF with CNN character representations \cite{Ma2016} \\
      lstm-crf-lstm & LSTM-CRF with LSTM character representations \cite{Lample2016} \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{Model descriptions}
  \label{tab:models}
\end{table}

Named entities were only considered to be correct if they were a complete match of the 
corresponding entity in the dataset. The models tested in both experiments are described in Table \ref{tab:models}.
In each experiment, we evaluated variants of each model using the complete set of features described in 
Table \ref{tab:features} and variants that used only lowercase unaccented words for HMMs, 
and only word embeddings in the case of neural models and CRFs. In Tables \ref{tab:experiment1} to 
\ref{tab:experiment2f}, models that used the complete set of features have the suffix "+F". For example,
"lstm-crf+F" is the LSTM-CRF model with all features including GloVe word embeddings, and "lstm-crf" is 
the LSTM-CRF model with only GloVe word embeddings as input.
Additionally, HMMs were also evaluated using the self training strategy. In tables \ref{tab:experiment1f} 
and \ref{tab:experiment2f}, these models have the suffix "+ST".

\subsection{Experiment 1: Complete Dataset}

\begin{table}[h]
  \small
  \begin{center}
    \begin{tabular}{ lllllll }
      \toprule
      \multirow{2}{*}{Model} & \multicolumn{3}{c}{Validation} & \multicolumn{3}{c}{Test} \\
                             & \multicolumn{1}{c}{P} & \multicolumn{1}{c}{R} & \multicolumn{1}{c}{F1}
                             & \multicolumn{1}{c}{P} & \multicolumn{1}{c}{R} & \multicolumn{1}{c}{F1} \\
      \midrule
      hmm-1	    & 0.6965 & 0.5749 & 0.6299 & 0.6263 & 0.4431 & 0.5190 \\
      hmm-2	    & 0.7047 & 0.6286 & 0.6645 & 0.6480 & 0.5222 & 0.5783 \\
      hmm-3	    & 0.6127 & 0.6141 & 0.6134 & 0.5471 & 0.4634 & 0.5018 \\
      crf	    & 0.7173 & 0.6683 & 0.6920 & 0.6671 & 0.5868 & 0.6244 \\
      lstm-crf	    & 0.8484 & 0.9044 & 0.8755 & 0.8358 & 0.8497 & 0.8427 \\
      lstm-crf-cnn  & 0.9058 & 0.9575 & 0.9309 & 0.8779 & 0.8737 & 0.8758 \\
      lstm-crf-lstm & 0.9134 & 0.9435 & 0.9282 & 0.8920 & 0.8815 & 0.8867 \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{Precision, recall and F1 in the complete dataset for models using no features}
  \label{tab:experiment1}
\end{table}

Experiment 1 consisted of testing the models in the complete NER on HTML dataset 
described in Section \ref{sec:ner_dataset}. Table \ref{tab:experiment1} shows the 
Precision (P), Recall (R), and F1-scores (F1) for models that employed no features, 
except for lowercase unaccented words
in the case of HMMs and Glove-100 word embeddings in the case of CRFs and Neural Models.

Without carefully designed features and a gazetteer, HMMs and CRFs have a very 
poor performance, achieving an F1-score of only 0.5783 for hmm-2 and 0.6244 for crf
in the test set. This is to be expected, as these models rely on a good feature selection.
The neural models however achieved high F1-scores in the test
data, even with the absence of features. The plain LSTM-CRF architecture improved performance 
significantly in comparison to the conventional CRF, 0.8427 against 0.6244 (a difference 
of 0.2183). Also, the neural character representations boosted performance a little further
reaching an F1-score of 0.8758 for CNN based representations and 0.8867 for LSTM based
representations. The LSTM based representations were superior in modelling
morphological features, perhaps because they are able to differentiate suffixes and prefixes, 
while CNNs filters are position invariant. 

The results in experiment 1 also show that pretrained word embeddings can work as a sort of
universal gazetteer. Words with similar embeddings, are likely to belong to the same class.
This knowledge combined with the ability to learn morphological features can make up for 
the scarcity of textual data in some webpages.

\begin{table}[h]
  \small
  \begin{center}
    \begin{tabular}{ lllllll }
      \toprule
      \multirow{2}{*}{Model} & \multicolumn{3}{c}{Validation} & \multicolumn{3}{c}{Test} \\
                             & \multicolumn{1}{c}{P} & \multicolumn{1}{c}{R} & \multicolumn{1}{c}{F1}
                             & \multicolumn{1}{c}{P} & \multicolumn{1}{c}{R} & \multicolumn{1}{c}{F1} \\
      \midrule
      hmm-1+F	      & 0.6061 & 0.7282 & 0.6616 & 0.7106 & 0.7633 & 0.7360 \\
      hmm-2+F	      & 0.6279 & 0.7550 & 0.6856 & 0.7521 & 0.7810 & 0.7663 \\
      hmm-3+F	      & 0.6573 & 0.7819 & 0.7142 & 0.7523 & 0.7795 & 0.7657 \\
      hmm-1+F+ST      & 0.7032 & 0.9077 & 0.7925 & 0.7522 & 0.8663 & 0.8052 \\
      hmm-2+F+ST      & 0.7321 & 0.9172 & 0.8143 & 0.7737 & 0.8789 & 0.8230 \\
      hmm-3+F+ST      & 0.7551 & 0.9172 & 0.8283 & 0.7961 & 0.8534 & 0.8237 \\
      crf+F	      & 0.9024 & 0.9049 & 0.9037 & 0.8751 & 0.8227 & 0.8481 \\
      lstm-crf+F      & 0.9430 & 0.9530 & 0.9480 & 0.8998 & 0.8527 & 0.8756 \\
      lstm-crf-cnn+F  & 0.9244 & 0.9715 & 0.9474 & 0.9017 & 0.8973 & 0.8995 \\
      lstm-crf-lstm+F & 0.9465 & 0.9692 & 0.9577 & 0.9108 & 0.8715 & 0.8907 \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{Precision, recall and F1 in the complete dataset for models using all features}
  \label{tab:experiment1f}
\end{table}

Table \ref{tab:experiment1f} shows the results on Experiment 1 for models
that employed all the features described in Table \ref{tab:features}.
Conventional models like HMMs, and CRFs can become competitive with
the right selection of features and a good gazetteer, however they still lose
to the best neural model without features, demonstrating their inherent limitations.
We can also see that the self training strategy for HMMs improved the quality of 
the models significantly in all cases, boosting both precision and recall. Finally,
HMMs that employed trigrams or quadrigrams (hmm-2, hmm-3) also performed better in
comparison to regular HMMs.
The neural models also improved a little with the new features. The plain LSTM-CRF
model gets a closer performance to the models that employed neural character 
representations. It points to the fact that the LSTM and CNN character representations 
were able to learn at least part of the morphological features automatically in the
models with no features. So, when these features are added explicitly, the differences 
in performance become less noticeable.

\subsection{Experiment 2: Reduced Dataset}

\begin{table}[h]
  \small
  \begin{center}
    \begin{tabular}{ lllllll }
      \toprule
      \multirow{2}{*}{Model} & \multicolumn{2}{c}{Precision} & \multicolumn{2}{c}{Recall} & \multicolumn{2}{c}{F1} \\
                             & \multicolumn{1}{c}{\tiny{M}} & \multicolumn{1}{c}{\tiny{SD}}
			     & \multicolumn{1}{c}{\tiny{M}} & \multicolumn{1}{c}{\tiny{SD}}
			     & \multicolumn{1}{c}{\tiny{M}} & \multicolumn{1}{c}{\tiny{SD}} \\
      \midrule
      hmm-1	      & 0.2856 & 0.1127 & 0.1623 & 0.0810 & 0.2036 & 0.0892 \\
      hmm-2	      & 0.3436 & 0.1134 & 0.2060 & 0.1058 & 0.2483 & 0.0997 \\
      hmm-3	      & 0.2707 & 0.0955 & 0.2163 & 0.0927 & 0.2316 & 0.0793 \\
      crf	      & 0.6249 & 0.0433 & 0.5272 & 0.0348 & 0.5711 & 0.0301 \\
      lstm-crf	      & 0.6982 & 0.0495 & 0.6538 & 0.0532 & 0.6751 & 0.0499 \\
      lstm-crf-cnn    & 0.7906 & 0.0557 & 0.7431 & 0.0687 & 0.7652 & 0.0572 \\
      lstm-crf-lstm   & 0.7935 & 0.0345 & 0.7360 & 0.0424 & 0.7626 & 0.0237 \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{Mean Precision, recall and F1 in the test set for models with no features
   trained in five sets with 17 documents}
  \label{tab:experiment2}
\end{table}

The second experiment aimed to evaluate the performance of sequence models when 
the amount of training data is small. The same training data used in Experiment 1 was
split into five non overlapping sets with 17 documents. Each model was trained once 
with each of the five sets and tested with the same test set used in Experiment 1. 
Seventeen documents seem to be a feasible amount of documents to label in most WDE 
tasks. The results for each model were averaged over the five runs.

Table \ref{tab:experiment2} shows the precision, recall and F1-score for the same models
used in Experiment 1, but in this case we only show results for the Test set.
As expected, we can see that the performance of all models with no features was reduced in comparison 
to Experiment 1. HMMs show a terrible performance while CRFs are not 
impacted as much by the reduction in the amount of training examples. Neural models
keep performing better, but they were unable to learn patterns as efficiently in
the smaller training sets, the best model achieves an F1 score of 0.7652 in contrast to
0.8815 with the entire dataset. The neural models show a small standard deviation, what
points to the fact that they can retain a consistent performance between different training 
datasets. In this matter, the dropout layers are crucial to produce better generalizations 
and prevent overfitting.

\begin{table}[h]
  \small
  \begin{center}
    \begin{tabular}{lllllll}
      \toprule
      \multirow{2}{*}{Model} & \multicolumn{2}{c}{Precision} & \multicolumn{2}{c}{Recall} & \multicolumn{2}{c}{F1} \\
                             & \multicolumn{1}{c}{\tiny{M}} & \multicolumn{1}{c}{\tiny{SD}}
			     & \multicolumn{1}{c}{\tiny{M}} & \multicolumn{1}{c}{\tiny{SD}}
			     & \multicolumn{1}{c}{\tiny{M}} & \multicolumn{1}{c}{\tiny{SD}} \\
      \midrule
      hmm-1+F	      & 0.6205 & 0.0927 & 0.6402 & 0.0717 & 0.6290 & 0.0774 \\
      hmm-2+F	      & 0.5999 & 0.0879 & 0.6527 & 0.0389 & 0.6229 & 0.0595 \\
      hmm-3+F	      & 0.5712 & 0.0957 & 0.6258 & 0.0494 & 0.5954 & 0.0722 \\
      hmm-1+F+ST      & 0.6536 & 0.0886 & 0.7579 & 0.1118 & 0.7012 & 0.0963 \\
      hmm-2+F+ST      & 0.7044 & 0.0726 & 0.7810 & 0.1039 & 0.7397 & 0.0834 \\
      hmm-3+F+ST      & 0.6619 & 0.0722 & 0.7174 & 0.0892 & 0.6879 & 0.0777 \\
      crf+F	      & 0.8419 & 0.0326 & 0.7530 & 0.0823 & 0.7933 & 0.0541 \\
      lstm-crf+F      & 0.8376 & 0.0225 & 0.7838 & 0.0518 & 0.8093 & 0.0345 \\
      lstm-crf-cnn+F  & 0.8542 & 0.0336 & 0.7820 & 0.0767 & 0.8149 & 0.0478 \\
      lstm-crf-lstm+F & 0.8554 & 0.0219 & 0.8013 & 0.0825 & 0.8255 & 0.0478 \\
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{Mean Precision, recall and F1 in the test set for models with all features
   trained in five sets with 17 documents}
  \label{tab:experiment2f}
\end{table}

Table \ref{tab:experiment2f} shows the results in the small training sets 
for models that employed all features. Take note that the difference between CRFs
and neural models was reduced with the reduction in the training set. However, the
addition of features impacted the performance of neural models much more that in 
Experiment 1. This indicates that the models with neural character representations
may need more data to extract good morphological features. However, they keep performing 
better than the other models, and have the lowest standard deviations.

Finally, all the results presented in this section considered a dataset with a minimum
amount of preprocessing and no trimming strategy to delimitate named entities. In many cases, 
for example, a model may miss a few named entities because it includes a punctuation sign
or an honorific at the start or end of a named entity. In fact, we can boost the F1-score up
by a few points with some simple heuristics. However, these strategies are specific to this
NER on HTML problem.

\section{Conclusion}

In this paper we compared the performance of multiple sequence models in the task of
NER on HTML, introducing a novel dataset that is publicly available. We showed that
neural networks can be employed effectively to solve this task while demanding no 
feature engineering (F1-score of 0.8867), and they can retain acceptable performance 
even when the training data is dramatically reduced (F1-score of 0.8255), even though 
in this case the feature selection becomes more meaningful. We found that there are 
two components to the most successful models: neural based character representations
constructed with convolutional neural networks or bidirectional LSTMs and joint modelling 
of the output labels provided by a linear chain CRF output layer. The effective recognition 
of named entities on HTML is an essential step in most general web data extraction methods. 
However it may also point to the possibility of constructing a truly flexible approach to
cross domain data extraction based on neural networks.

\bibliographystyle{unsrt}
\bibliography{bibfile}

\end{document}
